{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/yatindandi/anaconda3/envs/myenv/lib/python3.7/site-packages/gym\")\n",
    "sys.path.append(\"/data/yatind/anaconda3/lib/python3.7/site-packages (0.10.8)\")\n",
    "import gym\n",
    "import os\n",
    "import gc\n",
    "from lib import plotting\n",
    "from gym.wrappers import Monitor\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "screen = env.reset()\n",
    "plt.imshow(screen)\n",
    "plt.show()\n",
    "print(screen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(screen):\n",
    "    screen = screen[34:194,0:160]\n",
    "    resize = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Grayscale(),\n",
    "        T.Resize((84,84)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    screen = resize(screen)\n",
    "    #plt.imshow(screen.numpy()[0], cmap='gray')\n",
    "    #plt.show()\n",
    "    screen = np.uint8(screen)\n",
    "    screen = np.array(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen).float()\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = process(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 84, 84])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),nn.ReLU(),nn.Conv2d(32, 64, kernel_size=4, stride=2),nn.ReLU(),nn.Conv2d(64, 64, kernel_size=3, stride=1),nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 512),nn.ReLU(),nn.Linear(512, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = process(state)\n",
    "state = torch.cat(tuple([state] * 4), dim=1)\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(env,num_episodes,experiment_dir,replay_memory_size=500000,replay_memory_init_size=50000,update_target_estimator_every=10000,discount_factor=0.99,epsilon_start=1.0,epsilon_end=0.1,epsilon_decay_steps=500000,batch_size=32,\n",
    "record_video_every=50):\n",
    "    \n",
    "    replay_memory = []\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    #monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    #if not os.path.exists(monitor_path):\n",
    "        #os.makedirs(monitor_path)\n",
    "    \n",
    "    num_steps = 0\n",
    "    state = env.reset()\n",
    "    state = process(state)\n",
    "    state = torch.cat(tuple([state] * 4), dim=1)\n",
    "    \n",
    "    def get_action():\n",
    "        sample = random.random()\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * num_steps / epsilon_decay_steps)\n",
    "        if sample > epsilon:\n",
    "            with torch.no_grad():\n",
    "                return (policy_net(state.to(device)).max(1)[1].data[0]).to(torch.device(\"cpu\"))\n",
    "        else:\n",
    "            return random.randrange(4)\n",
    "        \n",
    "    for i in range(replay_memory_init_size):\n",
    "        action = get_action()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = process(next_state)\n",
    "        next_state = torch.cat((state[:,1:,:,:],next_state), dim=1)\n",
    "        replay_memory.append(Transition(state, torch.tensor([[action]], device= torch.device(\"cpu\"), dtype=torch.long), torch.tensor([reward], device=torch.device(\"cpu\"), dtype=torch.float), next_state, torch.tensor([not done], device= torch.device(\"cpu\"), dtype=torch.float)))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = process(state)\n",
    "            state = torch.cat([state] * 4, dim=1)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    #env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = process(state)\n",
    "        state = torch.cat(tuple([state] * 4), dim=1)\n",
    "        loss = None\n",
    "        for t in count():\n",
    "            if num_steps % update_target_estimator_every == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            action = get_action()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            num_steps+=1\n",
    "            next_state = process(next_state)\n",
    "            next_state = torch.cat((state[:,1:,:,:],next_state), dim=1)\n",
    "\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "            replay_memory.append(Transition(state, torch.tensor([[action]], device= torch.device(\"cpu\"), dtype=torch.long), torch.tensor([reward], device=torch.device(\"cpu\"), dtype=torch.float), next_state, torch.tensor([not done], device= torch.device(\"cpu\"), dtype=torch.float)))\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] +=1\n",
    "            \n",
    "            transitions = random.sample(replay_memory, batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            state_batch = (torch.cat(batch.state)).to(device)\n",
    "            action_batch = torch.cat(batch.action).to(device)\n",
    "            reward_batch = torch.cat(batch.reward).to(device)\n",
    "            next_state_batch = torch.cat(batch.next_state).to(device)\n",
    "            done_batch = torch.cat(batch.done).to(device)\n",
    "            state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "            q_next_batch = target_net(next_state_batch).max(1)[0].detach()\n",
    "            target_batch = reward_batch + discount_factor*done_batch*q_next_batch\n",
    "            \n",
    "            loss = F.smooth_l1_loss(state_action_values, target_batch.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in policy_net.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step()\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "            if(i_episode%1000):\n",
    "                torch.save({\n",
    "                    'episode' : i_episode//1000+1,\n",
    "                    'state_dict' : policy_net.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict()},\n",
    "                    'dqn{}.model'.format(i_episode//1000+1))\n",
    "                \n",
    "        yield num_steps, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 3.0\n"
     ]
    }
   ],
   "source": [
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "i = 0\n",
    "for t, stats in learn(env,num_episodes=10000,experiment_dir=experiment_dir,replay_memory_size=500000,replay_memory_init_size=50000,update_target_estimator_every=10000,discount_factor=0.99,epsilon_start=1.0,epsilon_end=0.1,epsilon_decay_steps=500000,batch_size=32,record_video_every=100):\n",
    "        if(i%100==0):    \n",
    "            print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))\n",
    "        i+=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
