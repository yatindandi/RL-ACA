{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "from lib import plotting\n",
    "from gym.wrappers import Monitor\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f229d1805c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "screen = env.reset()\n",
    "plt.imshow(screen)\n",
    "plt.show()\n",
    "print(screen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(screen):\n",
    "    screen = screen[34:194,0:160]\n",
    "    resize = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Grayscale(),\n",
    "        T.Resize((84,84)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    screen = resize(screen)\n",
    "    #plt.imshow(screen.numpy()[0], cmap='gray')\n",
    "    #plt.show()\n",
    "    screen = np.uint8(screen)\n",
    "    screen = np.array(screen, dtype=np.float32) / 255\n",
    "    screen = torch.Tensor(screen)/ 255.0\n",
    "    return resize(screen).unsqueeze(0).to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = process(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 84, 84])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=8, stride=4),nn.ReLU(),nn.Conv2d(16, 32, kernel_size=4, stride=2),nn.ReLU(),nn.Conv2d(32, 32, kernel_size=3, stride=1),nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(7*7*32, 256),nn.ReLU(),nn.Linear(256, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = process(state)\n",
    "state = torch.cat(tuple([state] * 4), dim=1)\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(env,num_episodes,experiment_dir,replay_memory_size=500000,replay_memory_init_size=50000,update_target_estimator_every=10000,discount_factor=0.99,epsilon_start=1.0,epsilon_end=0.1,epsilon_decay_steps=500000,batch_size=32,\n",
    "record_video_every=50):\n",
    "    \n",
    "    replay_memory = []\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "    \n",
    "    num_steps = 0\n",
    "    state = env.reset()\n",
    "    state = process(state)\n",
    "    state = torch.cat(tuple([state] * 4), dim=1)\n",
    "    \n",
    "    def get_action():\n",
    "        sample = random.random()\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * num_steps / epsilon_decay_steps)\n",
    "        if sample > epsilon:\n",
    "            with torch.no_grad():\n",
    "                return (policy_net(state.to(device)).max(1)[1].data[0]).to(torch.device(\"cpu\"))\n",
    "        else:\n",
    "            return random.randrange(4)\n",
    "        \n",
    "    for i in range(replay_memory_init_size):\n",
    "        action = get_action()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = process(next_state)\n",
    "        next_state = torch.cat((state[:,1:,:,:],next_state), dim=1)\n",
    "        replay_memory.append(Transition(state, torch.tensor([[action]], device= torch.device(\"cpu\"), dtype=torch.long), torch.tensor([reward], device=torch.device(\"cpu\"), dtype=torch.float), next_state, torch.tensor([not done], device= torch.device(\"cpu\"), dtype=torch.float)))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = process(state)\n",
    "            state = torch.cat([state] * 4, dim=1)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = process(state)\n",
    "        state = torch.cat(tuple([state] * 4), dim=1)\n",
    "        loss = None\n",
    "        \n",
    "        for t in count():\n",
    "            if num_steps % update_target_estimator_every == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            action = get_action()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            num_steps+=1\n",
    "            next_state = process(next_state)\n",
    "            next_state = torch.cat((state[:,1:,:,:],next_state), dim=1)\n",
    "\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "            replay_memory.append(Transition(state, torch.tensor([[action]], device= torch.device(\"cpu\"), dtype=torch.long), torch.tensor([reward], device=torch.device(\"cpu\"), dtype=torch.float), next_state, torch.tensor([not done], device= torch.device(\"cpu\"), dtype=torch.float)))\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] +=1\n",
    "            \n",
    "            transitions = random.sample(replay_memory, batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            state_batch = (torch.cat(batch.state)).to(device)\n",
    "            action_batch = torch.cat(batch.action).to(device)\n",
    "            reward_batch = torch.cat(batch.reward).to(device)\n",
    "            next_state_batch = torch.cat(batch.next_state).to(device)\n",
    "            done_batch = torch.cat(batch.done).to(device)\n",
    "            state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "            q_next_batch = target_net(next_state_batch).max(1)[0].detach()\n",
    "            target_batch = reward_batch + discount_factor*done_batch*q_next_batch\n",
    "            \n",
    "            loss = F.smooth_l1_loss(state_action_values, target_batch.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in policy_net.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step()\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        yield num_steps, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 7.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 8.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 5.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 5.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 5.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 5.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 5.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 5.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 5.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 5.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 8.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 3.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 2.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 4.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 1.0\n",
      "\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d0b6ff33105b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexperiment_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./experiments/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperiment_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplay_memory_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplay_memory_init_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdate_target_estimator_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecord_video_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8cf20a96bf63>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(env, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitor_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_callable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrecord_video_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/virtual-py3/lib/python3.5/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/virtual-py3/lib/python3.5/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/virtual-py3/lib/python3.5/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/virtual-py3/lib/python3.5/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/virtual-py3/lib/python3.5/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/virtual-py3/lib/python3.5/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/virtual-py3/lib/python3.5/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting ffmpeg with \"%s\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'setsid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#setsid not present on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1488\u001b[0m                             \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1491\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "for t, stats in learn(env,num_episodes=10000,experiment_dir=experiment_dir,replay_memory_size=200000,replay_memory_init_size=20000,update_target_estimator_every=10000,discount_factor=0.99,epsilon_start=1.0,epsilon_end=0.1,epsilon_decay_steps=500000,batch_size=16,record_video_every=200):\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
