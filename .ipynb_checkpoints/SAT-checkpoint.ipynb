{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier = tf.contrib.layers.xavier_initializer()\n",
    "zeros = tf.zeros_initializer()\n",
    "dim_hidden = 1024 #Dim of hidden layer\n",
    "vocabulary_size = 0 #TODO : FIT VOCABULARY SIZE\n",
    "embedding_size = 512 #Size of the word embeddings\n",
    "annotation_L = 196 #Annotation vectors are of the form L*D\n",
    "annotation_D = 512 \n",
    "max_timesteps = 16\n",
    "lamda = 1.0\n",
    "dropout1 = True\n",
    "dropout1_rate = 0.5\n",
    "dropout2 = True\n",
    "dropout2_rate = 0.5\n",
    "consider_z = True\n",
    "consider_y = True\n",
    "batch_size=196\n",
    "alpha_c = 1\n",
    "NULL = 1\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_c0_h0(annotations):\n",
    "    with tf.variable_scope('initial_c0_h0'):\n",
    "        feature_mean = tf.reduce_mean(annotations,axis=1)\n",
    "        c0 = tf.layers.dense(inputs=feature_mean,units=dim_hidden,activation=tf.nn.tanh,kernel_initializer=xavier)\n",
    "        h0 = tf.layers.dense(inputs=feature_mean,units=dim_hidden,activation=tf.nn.tanh,kernel_initializer=xavier)\n",
    "        return c0,h0 #N * dim_hidden both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDER CONSTRUCTION\n",
    "def soft_attention(annotations,h_prev,i):\n",
    "    with tf.variable_scope('soft_attention',reuse=(i != 0)):\n",
    "        #HANDLE WITH CARE\n",
    "        h_extend = tf.expand_dims(e,axis=1) # N * 1 * h\n",
    "        onemat = tf.ones(shape=[batch_size,annotation_L,1]) # N * L * 1\n",
    "        h_large = tf.matmul(onemat,h_extend) # N * L * h\n",
    "        joined = tf.reshape(tf.concat([annotations,h_large],axis=2),[-1,dim_hidden + annotation_D]) # N  L * D + h\n",
    "        e = tf.reshape(tf.layers.dense(inputs=joined,units=1,activation=tf.nn.relu,kernel_initializer=xavier),[-1,annotation_L])\n",
    "        #HANDLE WITH CARE ENDS\n",
    "        alpha = tf.nn.softmax(e) #dimension N * L\n",
    "        beta = tf.layers.dense(inputs=h_prev,units=1,activation=tf.nn.sigmoid) #N * 1\n",
    "        alpha_exp = tf.expand_dims(alpha,axis=1) #N * 1 * L\n",
    "        pre_gating = tf.reshape(tf.matmul(alpha,annotations),[-1,annotation_D]) # (N,1,L) * (N,L,D) = (N,1,D) reshaped to (N,D)\n",
    "        z = tf.multiply(beta,pre_gating) #N * 1 and N * D pointwise multiplication \n",
    "        return z,alpha,beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation(img_features):\n",
    "    with tf.variable_scope('get_annotation_vecs'):\n",
    "        W = tf.get_variable('W',[annotation_D,annotation_D])\n",
    "        features_flat = tf.reshape(img_features,[-1,annotation_D])\n",
    "        features_proj = tf.matmul(features_flat,W)\n",
    "        annotations = tf.reshape(features_proj,[-1,annotation_L,annotation_D])\n",
    "        return annotations # N * L * D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(one_hot,i):\n",
    "    with tf.variable_scope('word_embedding',reuse=(i != 0)):\n",
    "        word_embedding = tf.get_variable('word_embedding',[vocabulary_size,embedding_size],initializer=tf.random_uniform_initializer(minval=-1.0,maxval=1.0))\n",
    "        return tf.nn.embedding_lookup(word_embedding,one_hot)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(ey,h,z,training,i):\n",
    "    with tf.variable_scope('get_logits',reuse=(i != 0)):\n",
    "        logits = h\n",
    "        if dropout1:\n",
    "            logits = tf.layers.dropout(inputs=logits,rate=dropout1_rate,training=training)\n",
    "        logits = tf.layers.dense(inputs=logits,units=dim_embed,activation=None,kernel_initializer=xavier)\n",
    "        if consider_y:\n",
    "            logits += ey\n",
    "        if consider_z:\n",
    "            logits += tf.layers.dense(inputs=logits,units=dim_embed,activation=None,use_bias=False,kernel_initializer=xavier)\n",
    "        logits = tf.nn.tanh(logits)\n",
    "        if dropout2:\n",
    "            logits = tf.layers.dropout(inputs=logits,rate=dropout2_rate,training=training)\n",
    "        return tf.layers.dense(inputs=logits,units=dim_vocabulary,activation=None,kernel_initializer=xavier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_function(features,labels,mode,params):\n",
    "    mask = tf.to_float(tf.not_equal(labels, NULL))\n",
    "    img_features = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    a = get_annotation(img_features)\n",
    "    c,h = initial_c0_h0(a)\n",
    "    cp,hp = initial_c0_h0(a)#for prediction\n",
    "    cell = rnn.BasicLSTMCell(num_units=dim_hidden)\n",
    "    start = tf.zeros([batch_size])\n",
    "    x = word_embedding(labels,0)\n",
    "    lastword = word_embedding(start,1)\n",
    "    alphas = []\n",
    "    predictions = []\n",
    "    loss = 0.0\n",
    "    for t in range(max_timesteps):\n",
    "        z,alpha,beta = soft_attention(a,h,t)\n",
    "        alphas.append(alpha)\n",
    "        _,h = cell(tf.concat( [x[:,t,:], z],1),[c, h])\n",
    "        logits=get_logits(x[:,t,:],h,z,true,t)\n",
    "        loss += tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels[:, t+1],logits=logits)*mask[:, t+1] )\n",
    "    alphas = tf.transpose(tf.stack(alpha_list), (1, 0, 2))\n",
    "    alphas_all = tf.reduce_sum(alphas, 1)    \n",
    "    alpha_reg = alpha_c * tf.reduce_sum((max_timesteps/batch_size - alphas_all) ** 2)\n",
    "    loss += alpha_reg\n",
    "    loss = loss / tf.to_float(batch_size)\n",
    "    \n",
    "    for t in range(max_timesteps):\n",
    "        zp,alphap,betap = soft_attention(a,hp,1)\n",
    "        _,[cp,hp] = cell(tf.concat([lastword, zp],1),[cp, hp])\n",
    "        logitsp = get_logits(lastword,hp,zp,true,1)\n",
    "        prediction = tf.argmax(logitsp,axis=1)\n",
    "        predictions.append(prediction)\n",
    "        lastword = word_embedding(prediction,1)\n",
    "        \n",
    "    predictions = {\n",
    "        'sentences' : (np.array(predictions)).T\n",
    "    }\n",
    "    \n",
    "    eval_ops = {\n",
    "        'accuracy' : tf.metrics.accuracy(labels=labels[:,1:],predictions=predictions['sentences'])\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(predictions=predictions,mode=mode)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,loss=loss,train_op=train_op)\n",
    "    \n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,loss=loss,eval_metric_ops=eval_ops)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
